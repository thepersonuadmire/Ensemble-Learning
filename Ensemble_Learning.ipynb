{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZmhqz3GJpv4DLj95UJ4DZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thepersonuadmire/Ensemble-Learning/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "2H87bdHj97Ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Yes, Bagging can be used for regression problems. In this case, the predictions from multiple models are averaged to produce a final prediction."
      ],
      "metadata": {
        "id": "dqdYinjd-BNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Multiple model training involves training several models (e.g., in ensemble methods) and combining their predictions, while single model training involves training just one model to make predictions."
      ],
      "metadata": {
        "id": "4hdKxLdh-Gfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. In Random Forest, feature randomness refers to the practice of selecting a random subset of features for each split in the decision trees. This helps to reduce correlation among the trees and improves the model's generalization."
      ],
      "metadata": {
        "id": "uvCH27fm-Mvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The OOB score is an internal validation method for Bagging models. It uses the predictions from trees that did not see a particular sample during training to estimate the model's performance."
      ],
      "metadata": {
        "id": "rcjkY26i-QMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. The OOB score is an internal validation method for Bagging models. It uses the predictions from trees that did not see a particular sample during training to estimate the model's performance."
      ],
      "metadata": {
        "id": "Z6pIKD8c-WI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. A Bagging Classifier works by creating multiple subsets of the training data through bootstrapping (sampling with replacement), training a base model (e.g., decision tree) on each subset, and then aggregating the predictions (e.g., by majority voting) to make the final prediction."
      ],
      "metadata": {
        "id": "4kTLXwBP-dER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. The performance of a Bagging Classifier can be evaluated using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC, depending on the nature of the classification problem."
      ],
      "metadata": {
        "id": "zN-3B4Lv-g5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A Bagging Regressor works similarly to a Bagging Classifier but focuses on regression tasks. It creates multiple bootstrapped datasets, trains a regressor on each, and averages the predictions to produce the final output."
      ],
      "metadata": {
        "id": "lqrljfou-j6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. The main advantage of ensemble techniques is that they often provide better predictive performance than individual models by reducing variance (in Bagging) or bias (in Boosting)."
      ],
      "metadata": {
        "id": "JcB7-MM_-nhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. The main challenge of ensemble methods is increased computational complexity and the potential for overfitting if the base models are too complex or if the ensemble is not properly tuned."
      ],
      "metadata": {
        "id": "FdEBnL9i-skC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. The key idea behind ensemble techniques is to combine multiple models to improve overall performance, leveraging the strengths of each model while mitigating their weaknesses."
      ],
      "metadata": {
        "id": "-q1_SMRd-yUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. A Random Forest Classifier is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of their predictions for classification tasks."
      ],
      "metadata": {
        "id": "kSpbGGmL_BUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. The main types of ensemble techniques include Bagging, Boosting, Stacking, and Voting."
      ],
      "metadata": {
        "id": "BrBI2WmI_Ct_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Ensemble learning is a technique that combines multiple models to produce a single model that performs better than any individual model."
      ],
      "metadata": {
        "id": "F6wkd7w1_Fzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Ensemble methods should be avoided when the dataset is very small, as they may lead to overfitting, or when interpretability is crucial, as ensembles can be more complex to interpret than single models."
      ],
      "metadata": {
        "id": "oA5GAdVg_Lva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Bagging reduces overfitting by averaging the predictions of multiple models trained on different subsets of the data, which helps to smooth out the noise and variance in the predictions."
      ],
      "metadata": {
        "id": "3QVIbnz3_PaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Random Forest is better than a single Decision Tree because it reduces overfitting by averaging multiple trees, which leads to better generalization on unseen data."
      ],
      "metadata": {
        "id": "WjjcGmg2_Vee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Bootstrap sampling involves creating multiple subsets of the training data by sampling with replacement. This allows each model in the ensemble to learn from different data points, enhancing diversity and robustness."
      ],
      "metadata": {
        "id": "5BuHH-v0_Y0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Real-world applications include credit scoring, fraud detection, medical diagnosis, image classification, and natural language processing tasks."
      ],
      "metadata": {
        "id": "Cy5iSO6G_ctu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Bagging builds multiple models independently and combines their predictions, while Boosting builds models sequentially, where each model attempts to correct the errors of the previous one."
      ],
      "metadata": {
        "id": "pDntLmPG_gnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "HOqsnLu0_kMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21."
      ],
      "metadata": {
        "id": "whD0_HUx_pJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 )\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "L0x4Ssh3_qLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22."
      ],
      "metadata": {
        "id": "CNkFtfzL_tEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "E2e7riWA_uOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23."
      ],
      "metadata": {
        "id": "V51ITu-z_xqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "for i, score in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "zbNp9n8P_yo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24."
      ],
      "metadata": {
        "id": "yat9d7Ou_24S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "dt_pred = dt_reg.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "print(f\"Random Forest MSE: {rf_mse:.2f}\")\n",
        "print(f\"Decision Tree MSE: {dt_mse:.2f}\")"
      ],
      "metadata": {
        "id": "joWdh-33_4kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25."
      ],
      "metadata": {
        "id": "7qC2ZC1A_7Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Train Random Forest Classifier with OOB\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"OOB Score: {rf_clf.oob_score_:.2f}\")"
      ],
      "metadata": {
        "id": "asp_qmUR_8Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26."
      ],
      "metadata": {
        "id": "DAZa1-T3ADna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with SVM\n",
        "bagging_svm = BaggingClassifier(base_estimator=SVC(), n_estimators=50, random_state=42)\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Bx-1SxpLAEpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27."
      ],
      "metadata": {
        "id": "NR8Px0muAGxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifiers with different numbers of trees\n",
        "for n_trees in [10, 50, 100, 200]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with {n_trees} trees: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "qIXAetCCAH8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28."
      ],
      "metadata": {
        "id": "2JXJNffRALzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with Logistic Regression\n",
        "bagging_log_reg = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=50, random_state=42)\n",
        "bagging_log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate AUC\n",
        "y_pred_proba = bagging_log_reg.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"AUC Score: {auc_score:.2f}\")"
      ],
      "metadata": {
        "id": "vBBHscqgANZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29."
      ],
      "metadata": {
        "id": "1T9Ge724AXn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Analyze feature importance scores\n",
        "feature_importances = rf_reg.feature_importances_\n",
        "for i, score in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "VRnNGog-AZbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30."
      ],
      "metadata": {
        "id": "-FKgUm1qAb6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_cl = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.2f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "8c0GSLCMAddm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31."
      ],
      "metadata": {
        "id": "fEnizwohAfz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "BSAycH9hAg91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32."
      ],
      "metadata": {
        "id": "i4EiS3x6Ak16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressors with different numbers of base estimators\n",
        "for n_estimators in [10, 50, 100, 200]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with {n_estimators} estimators: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "LeaBy8oAAl6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33."
      ],
      "metadata": {
        "id": "b767NC0BAozU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and find misclassified samples\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "misclassified = X_test[y_pred != y_test]\n",
        "\n",
        "print(f\"Number of misclassified samples: {len(misclassified)}\")"
      ],
      "metadata": {
        "id": "kOvEGQTWAqG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34."
      ],
      "metadata": {
        "id": "ql4gNtKoAscC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.2f}\")\n",
        "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "HXhJA-ZCAtnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35."
      ],
      "metadata": {
        "id": "CajEqzP1Awi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute confusion matrix\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KW6IA3w1AyLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36."
      ],
      "metadata": {
        "id": "88IZJInKA0bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True)),\n",
        "    ('log_reg', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "yPVE-TegA1pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37."
      ],
      "metadata": {
        "id": "_enDYYJtA4H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances and sort them\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "indices = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for i in range(5):\n",
        "    print(f\"Feature {indices[i]}: {feature_importances[indices[i]]:.4f}\")"
      ],
      "metadata": {
        "id": "Tch9-YKTA5tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38."
      ],
      "metadata": {
        "id": "XR4JFndsA8GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit (X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "EnACZBKXA9k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39."
      ],
      "metadata": {
        "id": "ujzWBBeqBAUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifiers with different max_depth values\n",
        "for max_depth in [None, 5, 10, 20]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with max_depth={max_depth}: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "rIeeRVuKBBjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40."
      ],
      "metadata": {
        "id": "KR69SSrSBFj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with Decision Tree\n",
        "bagging_dt = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "dt_pred = bagging_dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# Train Bagging Regressor with KNeighbors\n",
        "bagging_knn = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "knn_pred = bagging_knn.predict(X_test)\n",
        "knn_mse = mean_squared_error(y_test, knn_pred)\n",
        "\n",
        "print(f\"Decision Tree Bagging MSE: {dt_mse:.2f}\")\n",
        "print(f\"KNeighbors Bagging MSE: {knn_mse:.2f}\")"
      ],
      "metadata": {
        "id": "aJMVjx8GBG6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41."
      ],
      "metadata": {
        "id": "o0ZHrsJeBJQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and evaluate ROC-AUC\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "G0DiKC46BKVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42."
      ],
      "metadata": {
        "id": "4vPRQnV9BM6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier (base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Evaluate performance using cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
        "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
        "print(f\"Mean Cross-Validation Score: {cv_scores.mean():.2f}\")"
      ],
      "metadata": {
        "id": "ri-43K80BO5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43."
      ],
      "metadata": {
        "id": "Ysap_HBjBRG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = rf_clf.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UeBIR_heBS2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44."
      ],
      "metadata": {
        "id": "FCn183lGBVIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
        "    ('log_reg', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "713m5amABWWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45."
      ],
      "metadata": {
        "id": "Uqy86GWRBYpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressors with different bootstrap samples\n",
        "for bootstrap in [True, False]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, bootstrap=bootstrap, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with bootstrap={bootstrap}: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "caBm3CN7BZ32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}